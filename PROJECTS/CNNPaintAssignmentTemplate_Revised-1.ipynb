{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIVQ3OeV4WWt"
   },
   "source": [
    "# # Part 1a: Using tensorflow_hub to Create a Painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-oy0GKk4WWv"
   },
   "outputs": [],
   "source": [
    "# we will be using tensorflow to complete this project\n",
    "import tensorflow as tf\n",
    "\n",
    "# the next two lines of code are to handle SSL validations in case those arise\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# this will be the image that you wish to be painted. Right now this is just assigned with the URL of that image.\n",
    "content_path = tf.keras.utils.get_file('photo-1501820488136-72669149e0d4', \n",
    "                                       'https://images.unsplash.com/photo-1501820488136-72669149e0d4')\n",
    "\n",
    "# this will be the image that you wish to paint the above image with. Right now this is just assigned with the URL of that image.\n",
    "style_path = tf.keras.utils.get_file('Vincent_van_gogh%2C_la_camera_da_letto%2C_1889%2C_02.jpg',\n",
    "                                     'https://upload.wikimedia.org/wikipedia/commons/8/8c/Vincent_van_gogh%2C_la_camera_da_letto%2C_1889%2C_02.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWhgUL1u4WWx"
   },
   "outputs": [],
   "source": [
    "# this method loads the path to our images and returns an actual image that we can use\n",
    "def load_img(path_to_img):\n",
    "\n",
    "  # Reads and outputs the entire contents of the input filename.\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "\n",
    "  # Detect whether an image is a BMP, GIF, JPEG, or PNG, and \n",
    "  # performs the appropriate operation to convert the input \n",
    "  # bytes string into a Tensor of type dtype\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "\n",
    "  # Convert image to dtype, scaling (MinMax Normalization) its values if needed.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  # Scale the image using the custom function we created\n",
    "  img = img_scaler(img)\n",
    "\n",
    "  # Adds a fourth dimension to the Tensor because\n",
    "  # the model requires a 4-dimensional Tensor\n",
    "  return img[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JquVoWf4WWx"
   },
   "outputs": [],
   "source": [
    "# this method scales the image accordingly.\n",
    "# ** NOTE ** try out a different max_dim value and see the differences in how the final picture appears!\n",
    "def img_scaler(image, max_dim = 1000):\n",
    "\n",
    "  # Casts a tensor to a new type.\n",
    "  original_shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "\n",
    "  # Creates a scale constant for the image\n",
    "  scale_ratio = max_dim / max(original_shape)\n",
    "\n",
    "  # Casts a tensor to a new type.\n",
    "  new_shape = tf.cast(original_shape * scale_ratio, tf.int32)\n",
    "\n",
    "  # Resizes the image based on the scaling constant generated above\n",
    "  return tf.image.resize(image, new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkM_cZD14WWy"
   },
   "outputs": [],
   "source": [
    "# this is our actual image that we want painted by utilizing the above method, and the parameter being the URL from the image that we want painted\n",
    "content_image = load_img(content_path)\n",
    "\n",
    "# this is our actual image that we want to style with by utilizing the above method, and the parameter being the URL from the image that we want to style with\n",
    "style_image = load_img(style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "RMRUuF1y4WWy",
    "outputId": "066390f4-4e23-4180-ffd6-344592ace9c7"
   },
   "outputs": [],
   "source": [
    "# this import is used for graphing and to show the images.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this sets the sizes of the figures that we want painted and the figure that we want it styled with\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# this shows the image that we want painted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(content_image[0])\n",
    "plt.title('Content Image')\n",
    "\n",
    "# this shows the image that we want to style with\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(style_image[0])\n",
    "plt.title('Style Image')\n",
    "\n",
    "# show the iamge\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV6P82AN4WWz"
   },
   "outputs": [],
   "source": [
    "# this import as a built in neural network that we will be utilizing\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load Magenta's Arbitrary Image Stylization network from TensorFlow Hub  \n",
    "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "\n",
    "# Pass content and style images as arguments in TensorFlow Constant object format\n",
    "stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "UiztyD5j4WW0",
    "outputId": "d7c827f5-4a89-4a24-b389-8830066a9b1a"
   },
   "outputs": [],
   "source": [
    "# Set the size of the plot figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot the stylized image\n",
    "plt.imshow(stylized_image[0])\n",
    "\n",
    "# Add title to the plot\n",
    "plt.title('Stylized Image')\n",
    "\n",
    "# Hide axes\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODvPYCSk4WW0"
   },
   "source": [
    "**NOTE**\n",
    "\n",
    "The higher the max_dim value the higher the higher the quality picture but the slower it takes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N88QI96h4WW0"
   },
   "source": [
    "# # Part 1b: Paint Your Own Picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fLlib2I4WW1"
   },
   "source": [
    "Now that you have seen how tensorflow_hub and CNN's can be utilized to paint random pictures on the Internet, it is your turn to paint a picture of your own that you have saved on your picture!\n",
    "\n",
    "This process will be exactly the same except for how to get the content_path and the style_path set.\n",
    "\n",
    "Instructions to upload your own content and style image:\n",
    "\n",
    "1.) Go to https://postimages.org/\n",
    "\n",
    "\n",
    "2.) Choose a content image (image that you want painted) to upload by selecting the \"Choose images\" button. (make sure the drop down options above that \"Choose images\" button say \"Do not resize my image\" and \"No expiration\".\n",
    "\n",
    "\n",
    "3.) Select the photo that you want to be uploaded.\n",
    "\n",
    "\n",
    "4.) Scroll down to the \"Direct link:\" textbox. Copy the ENTIRE link shown and paste that link as the second paramter in the content_path variable assignment.\n",
    "\n",
    "\n",
    "5.) In that copied link, go to the last '/' and copy and paste only what is after that last '/' as the parameter in your content_path assignment. For example: if your entire direct link is: https://i.postimg.cc/bNSFmd0F/mohawkME.jpg , you would copy ONLY the 'mohawkME.jpg' portion of the URL. Paste this as the first parameter in the content_path assignment. \n",
    "\n",
    "\n",
    "6.) Go back to PostImage. Select the \"Upload another image\" button that is near the top of the page underneath your uploaded image. \n",
    "\n",
    "\n",
    "7.) Do a quick Google search to find a style image that you want your picture to be styled with. Save this image to your computer. Make sure that it is a jpg.\n",
    "\n",
    "\n",
    "8.) Repeat Steps 2-5. Note that this is now for the style image. You will need to do the above steps for style_path.\n",
    "\n",
    "\n",
    "9.) Run through the code in 1a exactly the same just with the content_path and style_path URL's changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q6W-pwx4WW1"
   },
   "outputs": [],
   "source": [
    "# keep the imports the same\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# the next two lines of code are to handle SSL validations in case those arise\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# this will be the image that you wish to be painted. Right now this is just assigned with the URL of that image.\n",
    "content_path = tf.keras.utils.get_file('LB13.jpg', \n",
    "                                       'https://i.postimg.cc/JtwTMxWm/LB13.jpg')\n",
    "\n",
    "# this will be the image that you wish to paint the above image with. Right now this is just assigned with the URL of that image.\n",
    "style_path = tf.keras.utils.get_file('style.jpg',\n",
    "                                     'https://i.postimg.cc/8zK2YxJZ/style.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQzk5qhz4WW1"
   },
   "outputs": [],
   "source": [
    "# this method loads the path to our images and returns an actual image that we can use\n",
    "def load_img(path_to_img):\n",
    "\n",
    "  # Reads and outputs the entire contents of the input filename.\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "\n",
    "  # Detect whether an image is a BMP, GIF, JPEG, or PNG, and \n",
    "  # performs the appropriate operation to convert the input \n",
    "  # bytes string into a Tensor of type dtype\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "\n",
    "  # Convert image to dtype, scaling (MinMax Normalization) its values if needed.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  # Scale the image using the custom function we created\n",
    "  img = img_scaler(img)\n",
    "\n",
    "  # Adds a fourth dimension to the Tensor because\n",
    "  # the model requires a 4-dimensional Tensor\n",
    "  return img[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEw7FqeC4WW2"
   },
   "outputs": [],
   "source": [
    "# this method scales the image accordingly.\n",
    "# ** NOTE ** try out a different max_dim value and see the differences in how the final picture appears!\n",
    "def img_scaler(image, max_dim = 1000):\n",
    "\n",
    "  # Casts a tensor to a new type.\n",
    "  original_shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
    "\n",
    "  # Creates a scale constant for the image\n",
    "  scale_ratio = max_dim / max(original_shape)\n",
    "\n",
    "  # Casts a tensor to a new type.\n",
    "  new_shape = tf.cast(original_shape * scale_ratio, tf.int32)\n",
    "\n",
    "  # Resizes the image based on the scaling constant generated above\n",
    "  return tf.image.resize(image, new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBL8uuQf4WW2"
   },
   "outputs": [],
   "source": [
    "# this is our actual image that we want painted by utilizing the above method, and the parameter being the URL from the image that we want painted\n",
    "my_content_image = load_img(content_path)\n",
    "\n",
    "# this is our actual image that we want to style with by utilizing the above method, and the parameter being the URL from the image that we want to style with\n",
    "my_style_image = load_img(style_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "9I2NjFzp4WW2",
    "outputId": "3a063694-ab32-4933-f840-dfde11f5bf06"
   },
   "outputs": [],
   "source": [
    "# this import is used for graphing and to show the images.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this sets the sizes of the figures that we want painted and the figure that we want it styled with\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# this shows the image that we want painted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(my_content_image[0])\n",
    "plt.title('My Content Image')\n",
    "\n",
    "# this shows the image that we want to style with\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(my_style_image[0])\n",
    "plt.title('My Style Image')\n",
    "\n",
    "# show the iamge\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glhdBG9F4WW2"
   },
   "outputs": [],
   "source": [
    "# this import as a built in neural network that we will be utilizing\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load Magenta's Arbitrary Image Stylization network from TensorFlow Hub  \n",
    "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "\n",
    "# Pass content and style images as arguments in TensorFlow Constant object format\n",
    "my_stylized_image = hub_module(tf.constant(my_content_image), tf.constant(my_style_image))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzokVnXM4WW3"
   },
   "source": [
    "Run the below code and you will see your own customized image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "q2a6JcSO4WW3",
    "outputId": "6d376831-67d5-4a3d-cc43-364bbe58c316"
   },
   "outputs": [],
   "source": [
    "# Set the size of the plot figure\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Plot the stylized image\n",
    "plt.imshow(my_stylized_image[0])\n",
    "\n",
    "# Add title to the plot\n",
    "plt.title('My Stylized Image')\n",
    "\n",
    "# Hide axes\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf_81wbx4WW3"
   },
   "source": [
    "## Make Your Own CNN to Style an Image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY4bgpaC4WW3"
   },
   "source": [
    "In the above parts, we utilized tensorflow_hub. Now tensorflow_hub is great! However, it may be a little too great.... Becuase of how great it is the CNN is imported in for us so we do not have to do any work. Now is when the fun begins and instead of importing tensorflow_hub we will create our own CNN to style an image of our choosing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzmDV4Wa4WW3"
   },
   "outputs": [],
   "source": [
    "## Importing necessary libraries as needed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import functools\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBpPl-aFvjYC"
   },
   "source": [
    "Now we will load the images in again. You can use the same content and style images as before or use new ones with the same instructions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Ml1byiS3nsFB",
    "outputId": "22b0db4d-a3f6-413c-b95e-0ba2c181cbeb"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE  #####\n",
    "#define content_path_new\n",
    "#define style_path_new\n",
    "\n",
    "\n",
    "\n",
    "content_image = load_img(content_path_new)\n",
    "style_image = load_img(style_path_new)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(content_image[0])\n",
    "plt.title('Content Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(style_image[0])\n",
    "plt.title('Style Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0-OhMFKgqau"
   },
   "outputs": [],
   "source": [
    "#Here we are converting the pixels values from [0 , 1] to [0, 255].\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDPs8ZO6wmZf"
   },
   "source": [
    "In this next cell, we are loading in the VGG19 network which is a pretrained image classification network. \n",
    "\n",
    "Use the intermediate layers of the model to get the content and style representations of the image. Starting from the network's input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level featuresâ€”object parts like wheels or eyes. The intermediate layers are necessary to define the representation of content and style from the images. For an input image, try to match the corresponding style and content target representations at these intermediate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1V6YnO0fuU8"
   },
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDbzCdYTxEr_"
   },
   "source": [
    "To test that the network is working correctly, we can use it to predict what the content image is showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMGNrBaJfuSi"
   },
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOA0PQeVxSWB"
   },
   "source": [
    "Here we are printing all of the layers in the VGG19 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC2UUYU5fuQB"
   },
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "  print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GpAu1YIxibO"
   },
   "source": [
    "In this next step, we need to choose which of the layers we will be using for the neural network. In the following code block, we need to define *content_layers* and *style_layers* as a list of layers from the ones printed above. *content_layers* should have 1-3 layers for optimal output and *style_layers* can have as many as you want.\n",
    "\n",
    "For example: content_layers = ['block1_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbXK83XQfuHX"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE  #####\n",
    "#define content_layers\n",
    "#define style_layers                \n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KKqU_ISyowD"
   },
   "source": [
    "We now need to build the neural net with the layers selected. This utilizes Keras which is designed so you can easily extract the intermediate layer values using the Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OFQS1Lg4WW3"
   },
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "  \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n",
    "  # Load our model. Load pretrained VGG, trained on ImageNet data\n",
    "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "\n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "  model = tf.keras.Model([vgg.input], outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLfREXsCyb2H"
   },
   "source": [
    "Here we are printing information about the style layers you printed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTlPBKZyf5EP"
   },
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "#Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "  print(name)\n",
    "  print(\"  shape: \", output.numpy().shape)\n",
    "  print(\"  min: \", output.numpy().min())\n",
    "  print(\"  max: \", output.numpy().max())\n",
    "  print(\"  mean: \", output.numpy().mean())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYkWWalTzJEW"
   },
   "source": [
    "The content of an image is represented by the values of the intermediate feature maps.\n",
    "\n",
    "Now some math is done to create a [Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix) that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEqqeJsgf5Bu"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  input_shape = tf.shape(input_tensor)\n",
    "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "  return result/(num_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlCjm2RUzeOc"
   },
   "source": [
    "The following class will now build a model that returns the style and content tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smw4olLKf4-3"
   },
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "  def __init__(self, style_layers, content_layers):\n",
    "    super(StyleContentModel, self).__init__()\n",
    "    self.vgg = vgg_layers(style_layers + content_layers)\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    self.num_style_layers = len(style_layers)\n",
    "    self.vgg.trainable = False\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"Expects float input in [0,1]\"\n",
    "    inputs = inputs*255.0\n",
    "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "    outputs = self.vgg(preprocessed_input)\n",
    "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                      outputs[self.num_style_layers:])\n",
    "\n",
    "    style_outputs = [gram_matrix(style_output)\n",
    "                     for style_output in style_outputs]\n",
    "\n",
    "    content_dict = {content_name: value\n",
    "                    for content_name, value\n",
    "                    in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "    style_dict = {style_name: value\n",
    "                  for style_name, value\n",
    "                  in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "    return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkLV-YezzxOj"
   },
   "source": [
    "Run the above model on the content image provided to see the results of the layers as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KXaK-vDgAjn"
   },
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "  print(\"  \", name)\n",
    "  print(\"    shape: \", output.numpy().shape)\n",
    "  print(\"    min: \", output.numpy().min())\n",
    "  print(\"    max: \", output.numpy().max())\n",
    "  print(\"    mean: \", output.numpy().mean())\n",
    "  print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "  print(\"  \", name)\n",
    "  print(\"    shape: \", output.numpy().shape)\n",
    "  print(\"    min: \", output.numpy().min())\n",
    "  print(\"    max: \", output.numpy().max())\n",
    "  print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jwec8aZ30PwG"
   },
   "source": [
    "Set your style and content target values and then set image to be a TensorFlow variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn4EGHp4gAhf"
   },
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DecbAcE0aVs"
   },
   "source": [
    "Since this is a float image, define a function to keep the pixel values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKNqYtO9gUBm"
   },
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ1PQJB90c6s"
   },
   "source": [
    "We need to create an [optmizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) using Keras. The Adam optimizer is reccomended but feel free to try other ones and mess around with the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytupzIOn0M0T"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE  #####\n",
    "#define opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyJJKrdG1N5T"
   },
   "source": [
    "To optimize this, use a weighted combination of the two losses to get the total loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXGAmtbGgT56"
   },
   "outputs": [],
   "source": [
    "style_weight=1e-2\n",
    "content_weight=1e4\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_gznj6v1UKl"
   },
   "source": [
    "Use tf.GradientTape to update the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Yefp9uLgbkC"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "  with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image)\n",
    "    loss = style_content_loss(outputs)\n",
    "\n",
    "  grad = tape.gradient(loss, image)\n",
    "  opt.apply_gradients([(grad, image)])\n",
    "  image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OCuYUcA1Z9D"
   },
   "source": [
    "We can run an iteration with the *train_step* function. Then to print the image, we use the *tensor_to_image* function. Here is one iteration of our own neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDz2_NlLgbhi"
   },
   "outputs": [],
   "source": [
    "train_step(image)\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXN_l7a717Lh"
   },
   "source": [
    "The image is likely not altered significantly. Create a loop below to do multiple iterations. Run at least 5 iterations, and if your machine can handle it, see how more than that will affect the image! This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UUDAaRVh9PE"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE  #####\n",
    "#create a loop to run train_step at least 5 times on your image then display it at the end with tensor_to_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Takxzi5g2A7-"
   },
   "source": [
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br><br>\n",
    ">\n",
    "\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br><br>\n",
    ">\n",
    "\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br><br>\n",
    "> "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNNPaintAssignmentTemplate_Revised.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
